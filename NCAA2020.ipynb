{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NCAA 2020 \n\nThis unfinished script was going to be my submission to the NCAA March Madness 2020 competition before it was cancelled due to the coronavirus.  This competition has been run for the last several years.  Code taken from public notebooks is acknowledged throughout.\n\n## Objective\n\nThe objective of the competition is to predict the probability a particular basketball team will beat another.  The predictions were to be evaluated on the results of the March Madness competition set to take place in 2020, which was cancelled due to the coronavirus.  \n\nPredictions were to be made for every possible combination of teams for each of the teams in the finals, prior to the finals actually taking place.  Only the predictions made on the games actually played were to be evaluated.  The metric used in this competition is the logloss.  The logloss notably heavily penalises predictions which are very confident and wrong.\n\n## Model \n\nThe model initially used was the lightgbm gradient boosting model.  This model was chosen for it's predictive accuracy, training speed and ease of use.  I chose this model particularly because following the data science bowl competition I wanted to test the permutation importance method of feature selection using this dataset.  Information about permutation importance is available here:\n\nhttps://academic.oup.com/bioinformatics/article/26/10/1340/193348\n\nFor the initial stages of the competition I planned to use holdout sets created taking an entire year's results from the training set for validation purposes.  The previous years competition results seemed to indicate there were two types of competition results - one type where every team that was heavily favoured to win actually won, and the best models were trained using standard techniques.  The second type was the case where teams heavily favoured to lose actually won, and models that were trained to be resilient to outlying predictions won.  My plan was to create one of each, since you are allowed two final submissions.\n\n## Features\n\nThis script creates hundreds of features, and then attempts to use permutation importance to select the important ones.  There doesn't seem to be enough samples for permutation to work with this number of features - this problem was never rectified because the competition was cancelled.  \n\n## Post-processing\n\nThis was an important tool in prior years.  I experimented with a few different methods.  The results are summarised here:\n\nhttps://www.kaggle.com/jarnel/clipping-spline-experiment-on-test-predictions\n\n## Retrospective\n\nFollowing from the retrospective on my data science bowl competition submission, my version control is again lacking - however I did include notes on what I was intending to work on next.  Comparisons between my holdout set results and the private leaderboard for previous years, this submission may have been within the top 5%.","execution_count":null},{"metadata":{"_uuid":"758590fe-69a7-4f14-bc7d-cdebec6bfbd6","_cell_guid":"554fc19c-0211-44c7-988d-b26bf4a3de10","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tqdm as tqdm\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.metrics import log_loss, mean_absolute_error, mean_squared_error\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction","execution_count":null},{"metadata":{"_uuid":"90e2033e-368d-49b2-8185-65d21db8728a","_cell_guid":"d4dcd099-500d-4183-a4ff-25369203d4a5","trusted":true},"cell_type":"code","source":"# There are many files for this competition.  Most of this block is taken from a public kernel - it creates a dictionary containing all the data files.\n\ndata_dict = {}\nfor i in glob.glob('/kaggle/input/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/*'):\n    name = i.split('/')[-1].split('.')[0]\n    if name != 'MTeamSpellings':\n        data_dict[name] = pd.read_csv(i)\n    else:\n        data_dict[name] = pd.read_csv(i, encoding='cp1252')\ndata_dict.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create some score difference features.\n\nseason_result = data_dict['MRegularSeasonDetailedResults']\nrankings = data_dict['MMasseyOrdinals']\n\nseason_result['Score_difference_last'] = season_result['WScore'] - season_result['LScore']\nseason_result['score_diff_mean'] = season_result['Score_difference_last'].mean()\nseason_result['score_diff_med'] = season_result['Score_difference_last'].median()\nseason_result['score_diff_var'] = season_result['Score_difference_last'].var()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33c18418-ac78-4848-b13c-4ad8b67fed42","_cell_guid":"a76a2f40-544d-403c-aa11-a7fb297c67ac","trusted":true},"cell_type":"code","source":"# The training set size can be doubled by switching the teams.  The predictions are made as the probability team 1 beats team 2, swapping team 2 and team 1 creates an extra sample.\n\nseason_win_result = season_result[['Season', 'DayNum', 'LTeamID', 'WTeamID', 'WScore', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR',\n                                  'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'Score_difference_last', 'score_diff_mean', 'score_diff_med', 'score_diff_var']]\nseason_lose_result = season_result[['Season', 'DayNum', 'LTeamID', 'WTeamID', 'LScore', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA', 'LOR', 'LDR',\n                                   'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', 'Score_difference_last', 'score_diff_mean', 'score_diff_med', 'score_diff_var']]\nseason_lose_result['result'] = 0\nseason_win_result['result'] = 1\nseason_win_result.rename(columns={'WTeamID':'TeamID1', 'LTeamID':'TeamID2', 'WScore':'Score', 'WFGM':'FGM', 'WFGA':'FGA', 'WFGM3':'FGM3', 'WFGA3':'FGA3',\n                                  'WFTM':'FTM', 'WFTA':'FTA', 'WOR':'OR', 'WDR':'DR', 'WAst':'Ast', 'WTO':'TO', 'WStl':'Stl',\n                                  'WBlk':'Blk', 'WPF':'PF'}, inplace=True)\nseason_lose_result.rename(columns={'LTeamID':'TeamID1', 'WTeamID':'TeamID2', 'LScore':'Score', 'LFGM':'FGM', 'LFGA':'FGA', 'LFGM3':'FGM3', 'LFGA3':'FGA3',\n                                  'LFTM':'FTM', 'LFTA':'FTA', 'LOR':'OR', 'LDR':'DR', 'LAst':'Ast', 'LTO':'TO', 'LStl':'Stl',\n                                  'LBlk':'Blk', 'LPF':'PF'}, inplace=True)\nseason_lose_result['Score_difference_last'] = -season_lose_result['Score_difference_last']\nseason_lose_result['score_diff_mean'] = -season_lose_result['score_diff_mean']\nseason_lose_result['score_diff_med'] = -season_lose_result['score_diff_med']\nseason_lose_result['score_diff_var'] = -season_lose_result['score_diff_var']\nseason_result = pd.concat((season_win_result, season_lose_result)).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4d2ea1f-5a3e-4781-bbbc-61d16715bbf1","_cell_guid":"03c1b290-748f-4f25-9949-77a064692b69","trusted":true},"cell_type":"code","source":"# Creates a seed difference feature - the most important feature in every model I trained.  The seed is the ranking of the team going into the competition - \n# a large magnitude negative seed difference suggests a high probability of victory. \n\ntourney_result = data_dict['MNCAATourneyDetailedResults']\ntourney_seed = data_dict['MNCAATourneySeeds']\ntourney_result['Score_difference'] = tourney_result['WScore'] - tourney_result['LScore']\n\ntourney_result = tourney_result[['Season', 'WTeamID', 'LTeamID', 'Score_difference', 'DayNum']]\n\ntourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Seed':'WSeed'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)\ntourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Seed':'LSeed'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)\ntourney_result['WSeed'] = tourney_result['WSeed'].apply(lambda x: int(x[1:3]))\ntourney_result['LSeed'] = tourney_result['LSeed'].apply(lambda x: int(x[1:3]))\n\ntest_df = pd.read_csv('/kaggle/input/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MSampleSubmissionStage1_2020.csv')\ntest_df['Season'] = test_df['ID'].map(lambda x: int(x[:4]))\ntest_df['WTeamID'] = test_df['ID'].map(lambda x: int(x[5:9]))\ntest_df['LTeamID'] = test_df['ID'].map(lambda x: int(x[10:14]))\n\ntest_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Seed':'Seed1'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)\ntest_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Seed':'Seed2'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary statistics for the last 14 days of regular season games.\n\nlast_14_days = season_result.loc[season_result['DayNum'] >= 118].reset_index(drop=True)\nfor col in [x for x in last_14_days.columns if x not in ['Score_difference_last', 'TeamID1', 'TeamID2', 'Season', 'DayNum']]:\n    season_result_map_mean = last_14_days.groupby(['Season', 'TeamID1'])[col].mean().reset_index()\n    season_result_map_var = last_14_days.groupby(['Season', 'TeamID1'])[col].var().reset_index()\n    season_result_map_last = last_14_days.groupby(['Season', 'TeamID1'])[col].last().reset_index()\n    season_result_map_min = last_14_days.groupby(['Season', 'TeamID1'])[col].min().reset_index()\n    season_result_map_max = last_14_days.groupby(['Season', 'TeamID1'])[col].max().reset_index()\n    season_result_map_med = last_14_days.groupby(['Season', 'TeamID1'])[col].median().reset_index()\n\n    tourney_result = pd.merge(tourney_result, season_result_map_mean, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'W{col}14dMeanT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    tourney_result = pd.merge(tourney_result, season_result_map_mean, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'L{col}14dMeanT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    \n    tourney_result = pd.merge(tourney_result, season_result_map_var, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'W{col}14dVarT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    tourney_result = pd.merge(tourney_result, season_result_map_var, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'L{col}14dVarT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n        \n    tourney_result = pd.merge(tourney_result, season_result_map_last, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'W{col}14dLastT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    tourney_result = pd.merge(tourney_result, season_result_map_last, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'L{col}14dLastT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    \n    tourney_result = pd.merge(tourney_result, season_result_map_min, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'W{col}14dMinT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    tourney_result = pd.merge(tourney_result, season_result_map_min, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'L{col}14dMinT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    \n    tourney_result = pd.merge(tourney_result, season_result_map_max, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'W{col}14dMaxT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    tourney_result = pd.merge(tourney_result, season_result_map_max, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'L{col}14dMaxT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    \n    tourney_result = pd.merge(tourney_result, season_result_map_med, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'W{col}14dMedT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    tourney_result = pd.merge(tourney_result, season_result_map_med, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'L{col}14dMedT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    \n    test_df = pd.merge(test_df, season_result_map_mean, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'W{col}14dMeanT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    test_df = pd.merge(test_df, season_result_map_mean, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'L{col}14dMeanT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n\n    test_df = pd.merge(test_df, season_result_map_var, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'W{col}14dVarT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    test_df = pd.merge(test_df, season_result_map_var, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'L{col}14dVarT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    \n    test_df = pd.merge(test_df, season_result_map_last, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'W{col}14dLastT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    test_df = pd.merge(test_df, season_result_map_last, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'L{col}14dLastT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    \n    test_df = pd.merge(test_df, season_result_map_min, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'W{col}14dMinT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    test_df = pd.merge(test_df, season_result_map_min, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'L{col}14dMinT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    \n    test_df = pd.merge(test_df, season_result_map_max, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'W{col}14dMaxT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    test_df = pd.merge(test_df, season_result_map_max, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'L{col}14dMaxT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    \n    test_df = pd.merge(test_df, season_result_map_med, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'W{col}14dMedT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    test_df = pd.merge(test_df, season_result_map_med, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'L{col}14dMedT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6ff12a7-d54c-403e-906a-db086df818d7","_cell_guid":"dc04a7ea-73bf-40c1-a9bd-927d6c81f08b","trusted":true},"cell_type":"code","source":"# Summary statistics for past seasons.\n\nfor col in [x for x in season_result.columns if x not in ['Score_difference_last', 'TeamID1', 'TeamID2', 'Season', 'DayNum']]:\n    season_result_map_mean = season_result.groupby(['Season', 'TeamID1'])[col].mean().reset_index()\n    season_result_map_var = season_result.groupby(['Season', 'TeamID1'])[col].var().reset_index()\n    season_result_map_last = season_result.groupby(['Season', 'TeamID1'])[col].last().reset_index()\n    season_result_map_min = season_result.groupby(['Season', 'TeamID1'])[col].min().reset_index()\n    season_result_map_max = season_result.groupby(['Season', 'TeamID1'])[col].max().reset_index()\n    \n    tourney_result = pd.merge(tourney_result, season_result_map_mean, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'W{col}MeanT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    tourney_result = pd.merge(tourney_result, season_result_map_mean, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'L{col}MeanT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    \n    tourney_result = pd.merge(tourney_result, season_result_map_var, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'W{col}VarT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    tourney_result = pd.merge(tourney_result, season_result_map_var, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'L{col}VarT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n        \n    tourney_result = pd.merge(tourney_result, season_result_map_last, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'W{col}LastT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    tourney_result = pd.merge(tourney_result, season_result_map_last, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'L{col}LastT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    \n    tourney_result = pd.merge(tourney_result, season_result_map_min, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'W{col}MinT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    tourney_result = pd.merge(tourney_result, season_result_map_min, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'L{col}MinT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    \n    tourney_result = pd.merge(tourney_result, season_result_map_max, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'W{col}MaxT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    tourney_result = pd.merge(tourney_result, season_result_map_max, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    tourney_result.rename(columns={f'{col}':f'L{col}MaxT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID1', axis=1)\n    \n    test_df = pd.merge(test_df, season_result_map_mean, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'W{col}MeanT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    test_df = pd.merge(test_df, season_result_map_mean, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'L{col}MeanT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n\n    test_df = pd.merge(test_df, season_result_map_var, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'W{col}VarT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    test_df = pd.merge(test_df, season_result_map_var, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'L{col}VarT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    \n    test_df = pd.merge(test_df, season_result_map_last, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'W{col}LastT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    test_df = pd.merge(test_df, season_result_map_last, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'L{col}LastT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    \n    test_df = pd.merge(test_df, season_result_map_min, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'W{col}MinT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    test_df = pd.merge(test_df, season_result_map_min, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'L{col}MinT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    \n    test_df = pd.merge(test_df, season_result_map_max, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'W{col}MaxT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)\n    test_df = pd.merge(test_df, season_result_map_max, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID1'], how='left')\n    test_df.rename(columns={f'{col}':f'L{col}MaxT'}, inplace=True)\n    test_df = test_df.drop('TeamID1', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1cb9e2b-057c-444c-b319-50d224f669cf","_cell_guid":"5104b112-f6ab-424d-96d4-3d9cad641563","trusted":true},"cell_type":"code","source":"# Create features based on the various betting systems included in the datasets.\n\ninit_list = set(rankings['SystemName'].unique())\nfor season in tourney_result['Season'].unique():\n    systems = set(rankings.loc[rankings['Season'] == season]['SystemName'].unique())\n    init_list = init_list.intersection(systems)\nlen(init_list)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24960f91-18ac-447b-9415-a40bb6e79466","_cell_guid":"8b380b82-1836-4405-8170-28e4e661db33","trusted":true},"cell_type":"code","source":"# Summary statistics across different systems.\n\nfor systemname in init_list:\n    season_rankings_mean = rankings.loc[rankings['SystemName'] == systemname]\n    season_rankings_mean = season_rankings_mean.groupby(['Season', 'TeamID'])['OrdinalRank'].mean().reset_index()\n\n    tourney_result = pd.merge(tourney_result, season_rankings_mean, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n    tourney_result.rename(columns={'OrdinalRank':f'W{systemname}OrdinalRankT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID', axis=1)\n    tourney_result = pd.merge(tourney_result, season_rankings_mean, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n    tourney_result.rename(columns={'OrdinalRank':f'L{systemname}OrdinalRankT'}, inplace=True)\n    tourney_result = tourney_result.drop('TeamID', axis=1)\n\n    test_df = pd.merge(test_df, season_rankings_mean, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n    test_df.rename(columns={'OrdinalRank':f'W{systemname}OrdinalRankT'}, inplace=True)\n    test_df = test_df.drop('TeamID', axis=1)\n    test_df = pd.merge(test_df, season_rankings_mean, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n    test_df.rename(columns={'OrdinalRank':f'L{systemname}OrdinalRankT'}, inplace=True)\n    test_df = test_df.drop('TeamID', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d989d1b-e93c-444c-8a4d-677a3ec69436","_cell_guid":"42e12000-e3de-4ffd-929c-0b5424489d8e","trusted":true},"cell_type":"code","source":"# Feature renaming to make data usable in lightgbm.\n\ntourney_win_result = tourney_result.copy()\nfor col in [x for x in tourney_win_result.columns if x not in ['Season', 'DayNum', 'Score_difference']]:\n    if col[0] == 'W':\n        tourney_win_result.rename(columns={f'{col}':f'{col[1:]+\"1\"}'}, inplace=True)\n    elif col[0] == 'L':\n        tourney_win_result.rename(columns={f'{col}':f'{col[1:]+\"2\"}'}, inplace=True)\ntourney_lose_result = tourney_win_result.copy()\nfor col in tourney_lose_result.columns:\n    if col[-1] == '1':\n        col2 = col[:-1] + '2'\n        tourney_lose_result[col] = tourney_win_result[col2]\n        tourney_lose_result[col2] = tourney_win_result[col]\ntourney_lose_result.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1072fcfe-9537-415c-a1fa-140a7cccf91a","_cell_guid":"7fe692ac-1d09-4c9d-bbcf-311e088f2026","trusted":true},"cell_type":"code","source":"# Create difference features between teams.\n\ntourney_win_result['Seed_diff'] = tourney_win_result['Seed1'] - tourney_win_result['Seed2']\ntourney_win_result['ScoreMeanT_diff'] = tourney_win_result['ScoreMeanT1'] - tourney_win_result['ScoreMeanT2']\ntourney_win_result['ScoreVarT_diff'] = tourney_win_result['ScoreVarT1'] - tourney_win_result['ScoreVarT2']\n\ntourney_lose_result['Seed_diff'] = tourney_lose_result['Seed1'] - tourney_lose_result['Seed2']\ntourney_lose_result['ScoreMeanT_diff'] = tourney_lose_result['ScoreMeanT1'] - tourney_lose_result['ScoreMeanT2']\ntourney_lose_result['ScoreVarT_diff'] = tourney_lose_result['ScoreVarT1'] - tourney_lose_result['ScoreVarT2']\n\ntourney_lose_result['Score_difference'] = -tourney_lose_result['Score_difference']\ntourney_win_result['result'] = 1\ntourney_lose_result['result'] = 0\n\ntourney_result = pd.concat((tourney_win_result, tourney_lose_result)).reset_index(drop=True)\n\nfor col in test_df.columns[2:]:\n    if col[0] == 'W':\n        test_df.rename(columns={f'{col}':f'{col[1:]+\"1\"}'}, inplace=True)\n    elif col[0] == 'L':\n        test_df.rename(columns={f'{col}':f'{col[1:]+\"2\"}'}, inplace=True)\n\ntest_df['Seed1'] = test_df['Seed1'].apply(lambda x: int(x[1:3]))\ntest_df['Seed2'] = test_df['Seed2'].apply(lambda x: int(x[1:3]))\ntest_df['Seed_diff'] = test_df['Seed1'] - test_df['Seed2']\ntest_df['ScoreMeanT_diff'] = test_df['ScoreMeanT1'] - test_df['ScoreMeanT2']\ntest_df['ScoreVarT_diff'] = test_df['ScoreVarT1'] - test_df['ScoreVarT2']\ntest_df = test_df.drop(['ID', 'Pred'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# More feature renaming to make dataset usable.\n\nt1cols = [x for x in tourney_result.columns if 'OrdinalRankT1' in x]\nt2cols = [x for x in tourney_result.columns if 'OrdinalRankT2' in x]\ntourney_result['mean_rankt1'] = np.mean(tourney_result[t1cols], axis=1)\ntourney_result['mean_rankt2'] = np.mean(tourney_result[t2cols], axis=1)\ntest_df['mean_rankt1'] = np.mean(test_df[t1cols], axis=1)\ntest_df['mean_rankt2'] = np.mean(test_df[t2cols], axis=1)\nfor col in t1cols:\n    tourney_result[f'{col}_diff'] = tourney_result[col] - tourney_result[f'{col[:-1]+\"2\"}']\n    test_df[f'{col}_diff'] = test_df[col] - test_df[f'{col[:-1]+\"2\"}']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates a team 'quality' feature, which was useful in previous competitions.  Code taken from a public kernel.\n\nseason_result[['TeamID1', 'TeamID2']] = season_result[['TeamID1', 'TeamID2']].astype(str)\n\nimport statsmodels.api as sm\ndef team_quality(season):\n    formula = 'result~-1+TeamID1+TeamID2'\n    glm = sm.GLM.from_formula(formula=formula, \n                              data=season_result.loc[season_result['Season'] == season, ['TeamID1', 'TeamID2', 'result']].reset_index(drop=True), \n                              family=sm.families.Binomial()).fit()\n    \n    quality = pd.DataFrame(glm.params).reset_index()\n    quality.columns = ['TeamID','quality']\n    quality['Season'] = season\n    quality['quality'] = np.exp(quality['quality'])\n    quality = quality.loc[quality.TeamID.str.contains('ID1')].reset_index(drop=True)\n\n    quality['TeamID'] = quality['TeamID'].apply(lambda x: x[-5:-1]).astype(int)\n    return quality\n\nglm_quality = pd.concat([team_quality(x) for x in tourney_result['Season'].unique()]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge dataframes\n\ntourney_result = pd.merge(tourney_result, glm_quality, left_on=['Season', 'TeamID1'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'quality':'quality_T1'}, inplace=True)\ntourney_result = tourney_result.drop(columns='TeamID')\ntourney_result = pd.merge(tourney_result, glm_quality, left_on=['Season', 'TeamID2'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'quality':'quality_T2'}, inplace=True)\ntourney_result = tourney_result.drop(columns='TeamID')\n\ntest_df = pd.merge(test_df, glm_quality, left_on=['Season', 'TeamID1'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'quality':'quality_T1'}, inplace=True)\ntest_df = test_df.drop(columns='TeamID')\ntest_df = pd.merge(test_df, glm_quality, left_on=['Season', 'TeamID2'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'quality':'quality_T2'}, inplace=True)\ntest_df = test_df.drop(columns='TeamID')\n\ntest_df = test_df.drop(columns=['TeamID1', 'TeamID2'])\ntourney_result = tourney_result.drop(columns=['TeamID1', 'TeamID2'])\n\ntourney_result['winrate_diff'] = tourney_result['resultMeanT1'] - tourney_result['resultMeanT2']\ntest_df['winrate_diff'] = test_df['resultMeanT1'] - test_df['resultMeanT2']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c3d6791-50ae-46a9-a133-92cf33bdb97b","_cell_guid":"6bbf36cc-1b24-4945-ab4c-c3f63d262e42","trusted":true},"cell_type":"code","source":"# Create feature list.\n\nfor col in tourney_result.columns:\n    if col[-2:] == 'T1':\n        tourney_result[f'{col}_diff'] = tourney_result[col] - tourney_result[f'{col[:-1] + \"2\"}']\n        test_df[f'{col}_diff'] = test_df[col] - test_df[f'{col[:-1] + \"2\"}']\n        \nfeatures = [x for x in tourney_result.columns if x not in ['result', 'Score_difference', 'Season', 'DayNum']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training","execution_count":null},{"metadata":{"_uuid":"027254b7-d555-4d07-9167-a491e19db6c1","_cell_guid":"71bfb335-d016-48d8-8f37-04993e9c3d60","trusted":true},"cell_type":"code","source":"# The class NCAA_model creates a model, accepting several self-explanatory parameters.  Methods are available to train and predict using different post-processing methods.\n\nfrom scipy.optimize import minimize\nfrom scipy.interpolate import UnivariateSpline\nfrom functools import partial\nfrom bayes_opt import BayesianOptimization\n\ndef minimize_clipper(labels, preds, clips):\n    clipped = np.clip(preds, clips[0], clips[1])\n    return log_loss(labels, clipped)\n\ndef spline_model(labels, preds):\n    comb = pd.DataFrame({'labels':labels, 'preds':preds})\n    comb = comb.sort_values(by='preds').reset_index(drop=True)\n    spline_model = UnivariateSpline(comb['preds'].values, comb['labels'].values)\n    adjusted = spline_model(preds)\n    return spline_model, log_loss(labels, adjusted)\n\n\"\"\"\nIdeas:\n-custom loss function - large score differences attract the same penalty as small score differences\n-add team1 vs team2 history stats\n-for submission add p(x given x beat prev team) with dependence on bracket\n-player stats\n-adjust tournament winrates for late bracket games\n\"\"\"\n\nclass NCAA_model():\n    \n    def __init__(self, params, train_df, test_df, use_holdback=True, regression=False, verbose=True):\n        self.params = params\n        self.verbose = verbose\n        self.test_df = test_df\n        self.has_trained_models = False\n        self.models = []\n        if use_holdback == True:\n            self.use_holdback=2019\n        else:\n            self.use_holdback = use_holdback\n            \n        if regression:\n            self.params['objective'] = 'regression'\n            self.params['metric'] = 'mse'\n            self.target = 'Score_difference'\n            self.eval_func = mean_squared_error\n        else:\n            self.params['objective'] = 'binary'\n            self.params['metric'] = 'binary'\n            self.target = 'result'\n            self.eval_func = log_loss\n            \n        if not self.verbose:\n            self.params['verbosity'] = -1 \n            \n        if self.use_holdback:\n            self.holdback_df = train_df.query(f'Season == {self.use_holdback}')\n            self.holdback_target = self.holdback_df[self.target]\n            self.train_df = train_df.query(f'Season != {self.use_holdback}')\n        else:\n            self.train_df = train_df\n            \n        self.target = self.train_df[self.target]\n        \n    def train(self, features, n_splits, n_boost_round=5000, stopping_rounds=None, verbose_eval=1000):\n        self.feature_importances = pd.DataFrame(columns=features)\n        self.preds = np.zeros(shape=(self.test_df.shape[0]))\n        self.train_preds = np.zeros(shape=self.train_df.shape[0])\n        self.oof = np.zeros(shape=(self.train_df.shape[0]))\n        if self.use_holdback:\n            self.holdback_preds = np.zeros(shape=(self.holdback_df.shape[0]))\n        \n        cv = GroupKFold(n_splits=n_splits)        \n        for fold, (tr_idx, v_idx) in enumerate(cv.split(self.train_df, self.target, self.train_df['Season'])):\n            if self.verbose:\n                print(f'Fold: {fold}')\n                \n            x_train, y_train = self.train_df.iloc[tr_idx][features], self.target.iloc[tr_idx]\n            x_valid, y_valid = self.train_df.iloc[v_idx][features], self.target.iloc[v_idx]\n            X_t = lgb.Dataset(x_train, y_train)\n            X_v = lgb.Dataset(x_valid, y_valid)\n            \n            if self.has_trained_models:\n                self.models[fold] = lgb.train(self.params, X_t, num_boost_round = n_boost_round, early_stopping_rounds=stopping_rounds,\n                                                  valid_sets = [X_t, X_v], verbose_eval=(verbose_eval if self.verbose else None),\n                                                                                        init_model=self.models[fold])                \n            else:\n                model = lgb.train(self.params, X_t, num_boost_round = n_boost_round, early_stopping_rounds=stopping_rounds,\n                                                  valid_sets = [X_t, X_v], verbose_eval=(verbose_eval if self.verbose else None))\n                self.models.append(model)\n                \n            self.oof[v_idx] = self.models[fold].predict(x_valid)\n            self.train_preds[tr_idx] += self.models[fold].predict(x_train) / (n_splits-1)\n            self.preds += self.models[fold].predict(self.test_df[features]) / n_splits\n            self.feature_importances[f'fold_{fold}'] = self.models[fold].feature_importance()\n            if self.use_holdback:\n                self.holdback_preds += self.models[fold].predict(self.holdback_df[features]) / n_splits\n            \n            \n        tr_score = self.eval_func(self.target, self.train_preds)\n        oof_score = self.eval_func(self.target, self.oof)\n        self.has_trained_models = True\n        if self.verbose:\n            print(f'Training {self.params[\"metric\"]}: {tr_score}')\n            print(f'OOF {self.params[\"metric\"]}: {oof_score}')\n        if self.use_holdback:\n            hb_score = self.eval_func(self.holdback_target, self.holdback_preds)\n            if self.verbose:\n                print(f'Holdback set {self.params[\"metric\"]}: {hb_score}')\n            return tr_score, oof_score, hb_score\n        return tr_score, oof_score\n        \n    def fit_clipper(self, verbose=True):\n        preds = self.holdback_preds if self.use_holdback else self.oof\n        conv_target = np.where(self.holdback_target>0,1,0) if self.use_holdback else np.where(self.target>0,1,0)\n\n        partial_func = partial(minimize_clipper, conv_target, preds)\n        opt = minimize(partial_func, x0=[0.08, 0.92], method='nelder-mead')\n        if verbose:\n            print(f'Clip score: {opt.fun}')\n        clips = opt.x\n        score = opt.fun\n        return clips, score\n    \n    def fit_spline_model(self, verbose=True):\n        preds = self.holdback_preds if self.use_holdback else self.oof\n        conv_target = np.where(self.holdback_target>0,1,0) if self.use_holdback else np.where(self.target>0,1,0)\n        spline, score = spline_model(conv_target, preds)\n        if verbose:\n            print(f'Spline score: {score}')\n\n        return spline, score\n    \n    def postprocess_preds(self, opt_tool, method='clip', use_data='test', return_preds=False):\n        pred_dict = {'test':self.preds, 'train':self.train_preds, 'oof':self.oof}\n        label_dict = {'test':None, 'train':self.target, 'oof':self.target}       \n        if self.use_holdback:\n            pred_dict['hb'] = self.holdback_preds\n            label_dict['hb'] = self.holdback_target\n        if method == 'spline':\n            adjusted_preds = opt_tool(pred_dict[use_data])\n        elif method == 'clip':\n            adjusted_preds = np.clip(pred_dict[use_data], opt_tool[0], opt_tool[1])\n            \n        if use_data == 'test':\n            return adjusted_preds\n        if return_preds:\n            return adjusted_preds, self.eval_func(label_dict[use_data], adjusted_preds)\n        return self.eval_func(label_dict[use_data], adjusted_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Permutation Importance\n\nLarge chunks of code taken from public notebooks.  This is an early stage where I was investigating the different scoring methods, prior to the cancellation of the competition.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Null importance code.  The skeleton taken from a public notebook.  Modified for use in this competition.\n\nfrom sklearn.metrics import roc_auc_score\n\ndef get_feature_importances(data, shuffle, seed=None):\n    # Shuffle target if required\n    y = data['result'].copy()\n    if shuffle:\n        # Here you could as well use a binomial distribution\n        y = data['result'].copy().sample(frac=1.0)\n    params = {'num_leaves': 400,\n          'bagging_fraction': 0.6,\n          'feature_fraction':0.4,\n          'max_depth': -1,\n          'learning_rate': 0.005,\n          'bagging_freq':1,\n          \"boosting_type\": \"rf\",\n          \"bagging_seed\": 11,\n          'metric':'mse',\n          \"verbosity\": 0,\n          'random_state': 47,\n         }\n    models = []\n    cv = KFold(n_splits=10, random_state=0, shuffle=True)        \n    for fold, (tr_idx, v_idx) in enumerate(cv.split(data)):\n        x_train, y_train = data.iloc[tr_idx][features], y.iloc[tr_idx]\n        x_valid, y_valid = data.iloc[v_idx][features], y.iloc[v_idx]\n        X_t = lgb.Dataset(x_train, y_train)\n        X_v = lgb.Dataset(x_valid, y_valid)\n        model = lgb.train(params, X_t, num_boost_round = 15000, early_stopping_rounds=50,\n                                          valid_sets = [X_t, X_v], verbose_eval=None)\n        models.append(model)\n    # Get feature importances\n    imp_df = pd.DataFrame()\n    imp_df[\"feature\"] = list(features)\n    imp_df[\"importance_gain\"] = np.mean([x.feature_importance(importance_type='gain') for x in models], axis=0)\n    imp_df[\"importance_split\"] = np.mean([x.feature_importance(importance_type='split') for x in models], axis=0)\n    imp_df['trn_score'] = log_loss(y, sum([x.predict(data[features]) for x in models])/10)\n    \n    return imp_df\n\n# Seed the unexpected randomness of this world\nnp.random.seed(123)\n# Get the actual importance, i.e. without shuffling\nactual_imp_df = get_feature_importances(data=tourney_result, shuffle=False)\nprint(actual_imp_df.head())\n\nnull_imp_df = pd.DataFrame()\nnb_runs = 80\nfor i in range(nb_runs):\n    # Get current run importances\n    imp_df = get_feature_importances(data=tourney_result, shuffle=True)\n    imp_df['run'] = i + 1 \n    # Concat the latest importances with the old ones\n    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n    \nprint(null_imp_df.head())\n\nnull_imp_df.to_csv('null_importances_distribution_rf.csv')\nactual_imp_df.to_csv('actual_importances_ditribution_rf.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Measure the difference between the null distribution and the measured importance with labels that weren't permuted.\n\nfrom scipy.stats import entropy\nentropies_gain = []\nentropies_spl = []\nfor feature in actual_imp_df['feature'].unique():\n    entropies_gain.append(entropy(actual_imp_df.loc[actual_imp_df['feature']==feature]['importance_gain'].values, null_imp_df.loc[null_imp_df['feature']==feature]['importance_gain'].values))\n    entropies_spl.append(entropy(actual_imp_df.loc[actual_imp_df['feature']==feature]['importance_split'].values, null_imp_df.loc[null_imp_df['feature']==feature]['importance_split'].values))\nentropy_df = pd.DataFrame()\nentropy_df['feature'] = actual_imp_df['feature'].unique()\nentropy_df['gain'] = entropies_gain\nentropy_df['split'] = entropies_spl\nentropy_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display function.  Taken from public notebook.\n\ndef display_distributions(actual_imp_df_, null_imp_df_, feature_):\n    plt.figure(figsize=(13, 6))\n    gs = gridspec.GridSpec(1, 2)\n    # Plot Split importances\n    ax = plt.subplot(gs[0, 0])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper())\n    # Plot Gain importances\n    ax = plt.subplot(gs[0, 1])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of what the permutation importance does.\n\ndisplay_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='Seed1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Different scoring method, taken from public notebook.\n\nfeature_scores = []\nfor _f in actual_imp_df['feature'].unique():\n    f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n    f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n    gain_score = np.log(1e-10 + f_act_imps_gain / (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n    f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n    f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n    split_score = np.log(1e-10 + f_act_imps_split / (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n    feature_scores.append((_f, split_score, gain_score))\n\nscores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n\nplt.figure(figsize=(16, 16))\ngs = gridspec.GridSpec(1, 2)\n# Plot Split importances\nax = plt.subplot(gs[0, 0])\nsns.barplot(x='split_score', y='feature', data=scores_df.sort_values('split_score', ascending=False).iloc[0:100], ax=ax)\nax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n# Plot Gain importances\nax = plt.subplot(gs[0, 1])\nsns.barplot(x='gain_score', y='feature', data=scores_df.sort_values('gain_score', ascending=False).iloc[0:100], ax=ax)\nax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d8ba88f-8f72-498e-9e02-4d42742359e9","_cell_guid":"b66dfbbf-2b07-43be-856d-30f93bcc6353","trusted":true},"cell_type":"code","source":"# Different scoring method, taken from public notebook.\n\ncorrelation_scores = []\nfor _f in actual_imp_df['feature'].unique():\n    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].values\n    gain_score = 100 * (f_null_imps < np.percentile(f_act_imps, 5)).sum() / f_null_imps.size\n    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].values\n    split_score = 100 * (f_null_imps < np.percentile(f_act_imps, 5)).sum() / f_null_imps.size\n    correlation_scores.append((_f, split_score, gain_score))\n\ncorr_scores_df = pd.DataFrame(correlation_scores, columns=['feature', 'split_score', 'gain_score'])\n\nfig = plt.figure(figsize=(16, 16))\ngs = gridspec.GridSpec(1, 2)\n# Plot Split importances\nax = plt.subplot(gs[0, 0])\nsns.barplot(x='split_score', y='feature', data=corr_scores_df.sort_values('split_score', ascending=False).iloc[0:100], ax=ax)\nax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n# Plot Gain importances\nax = plt.subplot(gs[0, 1])\nsns.barplot(x='gain_score', y='feature', data=corr_scores_df.sort_values('gain_score', ascending=False).iloc[0:100], ax=ax)\nax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\nplt.tight_layout()\nplt.suptitle(\"Features' split and gain scores\", fontweight='bold', fontsize=16)\nfig.subplots_adjust(top=0.93)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Historical scores for comparison with my local CV.\n\nHistorical targets:\n2019 - 0.43\n2018 - 0.53\n2017 - 0.44\n2016 - 0.48\n2015 - 0.44\n\npossibly remove 2018","execution_count":null},{"metadata":{"_uuid":"458c8f94-6646-40f3-a037-664e01ef7e6a","_cell_guid":"b5272531-f67c-4e31-855e-6c0c8c41a88d","trusted":true},"cell_type":"code","source":"# Parameter search.  \n\ndef get_opt_params(init_points, n_iter, use_multiple_years=None):\n    \n    def run_parameter_opt(num_leaves, feature_fraction, bagging_fraction, min_data_in_leaf, max_depth, lambda_l1, lambda_l2,\n                          min_split_gain, min_child_weight, learning_rate):\n        params = {'num_leaves': int(num_leaves),\n              'min_child_weight': min_child_weight,\n              'min_split_gain' : min_split_gain,\n              'feature_fraction': feature_fraction,\n              'bagging_fraction': bagging_fraction,\n              'min_data_in_leaf': int(min_data_in_leaf),\n              'max_depth': int(max_depth),\n              'learning_rate': learning_rate,\n              'lambda_l1': lambda_l1,\n              'lambda_l2': lambda_l2,\n              'random_state': 7,\n              'boosting_type': 'gbdt',\n              'bagging_seed': 0,\n        }\n        if not use_multiple_years:\n            model = NCAA_model(params, tourney_result, test_df, use_holdback=[2019], regression=False, verbose=False)   \n            tr_score, oof_score, hb_score = model.train(features, n_splits=10, n_boost_round=10000, stopping_rounds=100)\n            spline, spline_s = model.fit_spline_model(verbose=False)\n           # return -spline_s\n            return -hb_score\n        spline_scores = []\n        hb_scores = []\n        for year in use_multiple_years:\n            model = NCAA_model(params, tourney_result, test_df, use_holdback=[year], regression=True, verbose=False)   \n            tr_score, oof_score, hb_score = model.train(features, n_splits=10, n_boost_round=10000, stopping_rounds=100)\n            spline, spline_s = model.fit_spline_model(verbose=False)\n            spline_scores.append(spline_s)\n            hb_scores.append(hb_score)\n        return -np.mean(spline_scores)\n        return -np.mean(hb_scores)\n    lgbBO = BayesianOptimization(run_parameter_opt, {'num_leaves': (15, 1000),\n                                        'feature_fraction': (0.7, 1),#.4515\n                                        'bagging_fraction': (0.5, 1),\n                                        'min_data_in_leaf': (20, 300),\n                                        'max_depth': (-1, 35),\n                                        'lambda_l1': (0, 3),\n                                        'lambda_l2': (0, 3),\n                                        'min_split_gain': (0, 0.3),\n                                        'min_child_weight': (0, 0.5),\n                                        'learning_rate': (0.001, 0.75)})\n    lgbBO.maximize(init_points=init_points, n_iter=n_iter)\n    \n    return lgbBO.max['params']\n\nparams = get_opt_params(init_points=5, n_iter=5, use_multiple_years=[2015, 2017, 2019])#4791\nparams['random_state'] = 7\nparams['boosting_type'] = 'gbdt'\nparams['bagging_seed'] =  0\nparams['min_data_in_leaf'] =  int(params['min_data_in_leaf'])\nparams['max_depth'] =  int(params['max_depth'])\nparams['num_leaves'] =  int(params['num_leaves'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the best number of features to remove.  Features are removed in order of their importance after accounting for the null importance distribution.\n\nfrom scipy.stats import norm\n\ndef score_feature_selection(df=None, train_features=None, cat_feats=None, target=None, use_multiple_years=None):\n    # Fit LightGBM \n    if not use_multiple_years:\n        model = NCAA_model(params, tourney_result, test_df, use_holdback=[2019], regression=False, verbose=False)   \n        tr_score, oof_score, hb_score = model.train(train_features, n_splits=10, n_boost_round=1500, stopping_rounds=100)\n        spline, spline_s = model.fit_spline_model(verbose=False)\n        return spline_s\n    spline_scores = []\n    hb_scores = []\n    for year in use_multiple_years:\n        model = NCAA_model(params, tourney_result, test_df, use_holdback=[year], regression=False, verbose=False)   \n        tr_score, oof_score, hb_score = model.train(train_features, n_splits=10, n_boost_round=1500, stopping_rounds=100)\n        spline, spline_s = model.fit_spline_model(verbose=False)\n        spline_scores.append(spline_s)\n        hb_scores.append(hb_score)\n    return np.mean(hb_scores)\n\nbase_score = 1\n\ngain_feats = list(entropy_df.sort_values(by='gain', ascending=False)['feature'].values)\nsplit_feats = list(entropy_df.sort_values(by='split', ascending=False)['feature'].values)\nshape = len(features)\nsteps = 50\nfor step in range(1,steps):\n    reduced_gain_feats = gain_feats[:int(step*(shape/steps))]\n    reduced_split_feats = split_feats[:int(step*(shape/steps))]        \n            \n    inters = sorted(list(set(reduced_split_feats).intersection(set(reduced_gain_feats))))\n    union_set = sorted(list(set(reduced_split_feats).union(set(reduced_gain_feats))))\n    print(len(reduced_split_feats), len(reduced_gain_feats), len(inters), len(union_set))\n    print(f'Results for threshold {step}')\n    split_results = score_feature_selection(df=tourney_result, train_features=reduced_split_feats, cat_feats=None, target=tourney_result['result'], use_multiple_years=[2015, 2017, 2019])\n    print(f'split set: {split_results}')\n    gain_results = score_feature_selection(df=tourney_result, train_features=reduced_gain_feats, cat_feats=None, target=tourney_result['result'], use_multiple_years=[2015, 2017, 2019])\n    print(f'gain set: {gain_results}')\n    try:\n        intersection_results = score_feature_selection(df=tourney_result, train_features=inters, cat_feats=None, target=tourney_result['result'], use_multiple_years=[2015, 2017, 2019])\n        print(f'intersection set: {intersection_results}')\n    except ValueError:\n        pass\n    union_results = score_feature_selection(df=tourney_result, train_features=union_set, cat_feats=None, target=tourney_result['result'], use_multiple_years=[2015, 2017, 2019])\n    print(f'union set: {union_results}')\n    \n    if split_results < base_score:\n        reduced_feats = reduced_split_feats\n        base_score = split_results\n    if gain_results < base_score:\n        reduced_feats = reduced_gain_feats\n        base_score = gain_results\n    if intersection_results < base_score:\n        reduced_feats = inters\n        base_score = intersection_results\n    if union_results < base_score:\n        reduced_feats = union_set\n        base_score = union_results\n    print(base_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Same as above cell, with different scoring method.\n\ndef score_feature_selection(df=None, train_features=None, cat_feats=None, target=None, use_multiple_years=None):\n    # Fit LightGBM \n    if not use_multiple_years:\n        model = NCAA_model(params, tourney_result, test_df, use_holdback=[2019], regression=False, verbose=False)   \n        tr_score, oof_score, hb_score = model.train(train_features, n_splits=10, n_boost_round=1500, stopping_rounds=100)\n        spline, spline_s = model.fit_spline_model(verbose=False)\n        return spline_s\n    spline_scores = []\n    hb_scores = []\n    for year in use_multiple_years:\n        model = NCAA_model(params, tourney_result, test_df, use_holdback=[year], regression=False, verbose=False)   \n        tr_score, oof_score, hb_score = model.train(train_features, n_splits=10, n_boost_round=1500, stopping_rounds=100)\n        spline, spline_s = model.fit_spline_model(verbose=False)\n        spline_scores.append(spline_s)\n        hb_scores.append(hb_score)\n    return np.mean(hb_scores)\n\nbase_score = 1\n\nscores = []\n\nfor threshold in range(1, 341):\n    reduced_split_feats = scores_df.sort_values(by='split_score', ascending=False)['feature'].values[:threshold]\n    reduced_gain_feats = scores_df.sort_values(by='gain_score', ascending=False)['feature'].values[:threshold]\n    \n    inters = sorted(list(set(reduced_split_feats).intersection(set(reduced_gain_feats))))\n    union_set = sorted(list(set(reduced_split_feats).union(set(reduced_gain_feats))))\n    print(len(reduced_split_feats), len(reduced_gain_feats), len(inters), len(union_set))\n    print(f'Results for threshold {threshold}')\n    split_results = score_feature_selection(df=tourney_result, train_features=reduced_split_feats, cat_feats=None, target=tourney_result['result'], use_multiple_years=[2015, 2017, 2019])\n    print(f'split set: {split_results}')\n    gain_results = score_feature_selection(df=tourney_result, train_features=reduced_gain_feats, cat_feats=None, target=tourney_result['result'], use_multiple_years=[2015, 2017, 2019])\n    print(f'gain set: {gain_results}')\n    union_results = score_feature_selection(df=tourney_result, train_features=union_set, cat_feats=None, target=tourney_result['result'], use_multiple_years=[2015, 2017, 2019])\n    print(f'union set: {union_results}')\n\n    try:\n        intersection_results = score_feature_selection(df=tourney_result, train_features=inters, cat_feats=None, target=tourney_result['result'], use_multiple_years=[2015, 2017, 2019])\n        print(f'intersection set: {intersection_results}')\n        if intersection_results < base_score:\n            reduced_feats = inters\n            base_score = intersection_results\n            scores.append([split_results, gain_results, intersection_results, union_results])\n    except ValueError:\n        pass\n    \n    if split_results < base_score:\n        reduced_feats = reduced_split_feats\n        base_score = split_results\n    if gain_results < base_score:\n        reduced_feats = reduced_gain_feats\n        base_score = gain_results\n    if union_results < base_score:\n        reduced_feats = union_set\n        base_score = union_results\n    print(base_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Same as above cell, with different scoring method.\n\nfrom scipy.stats import norm\n\ndef score_feature_selection(df=None, train_features=None, cat_feats=None, target=None, use_multiple_years=None):\n    # Fit LightGBM \n    if not use_multiple_years:\n        model = NCAA_model(params, tourney_result, test_df, use_holdback=[2019], regression=False, verbose=False)   \n        tr_score, oof_score, hb_score = model.train(train_features, n_splits=10, n_boost_round=1500, stopping_rounds=100)\n        spline, spline_s = model.fit_spline_model(verbose=False)\n        return spline_s\n    spline_scores = []\n    for year in use_multiple_years:\n        model = NCAA_model(params, tourney_result, test_df, use_holdback=[year], regression=False, verbose=False)   \n        tr_score, oof_score, hb_score = model.train(train_features, n_splits=10, n_boost_round=1500, stopping_rounds=100)\n        spline, spline_s = model.fit_spline_model(verbose=False)\n        spline_scores.append(spline_s)\n    return np.mean(spline_scores)\n\nbase_score = 1\n\nnull_f_std = null_imp_df.groupby('feature')['importance_gain'].std().reset_index()\nnull_f_mean = null_imp_df.groupby('feature')['importance_gain'].mean().reset_index()\nact_f_mean = actual_imp_df.groupby('feature')['importance_gain'].mean().reset_index()\nact_f_mean['p values gain'] = 1 - norm.cdf(act_f_mean['importance_gain'], null_f_mean['importance_gain'], null_f_std['importance_gain'])\nnull_f_std = null_imp_df.groupby('feature')['importance_split'].std().reset_index()\nnull_f_mean = null_imp_df.groupby('feature')['importance_split'].mean().reset_index()\nact_f_mean_spl = actual_imp_df.groupby('feature')['importance_split'].mean().reset_index()\nact_f_mean['p values split'] = 1 - norm.cdf(act_f_mean_spl['importance_split'], null_f_mean['importance_split'], null_f_std['importance_split'])\nbottom = np.min(act_f_mean[['p values split', 'p values gain']])\nprint(len(features))\n\ngain_feats = list(act_f_mean.sort_values(by='p values gain', ascending=True)['feature'].values)\nsplit_feats = list(act_f_mean.sort_values(by='p values split', ascending=True)['feature'].values)\nshape = len(features)\nsteps = 96\nfor step in range(1,steps):\n    reduced_gain_feats = gain_feats[:int(step*(shape/steps))]\n    reduced_split_feats = split_feats[:int(step*(shape/steps))]\n\n\n    inters = sorted(list(set(reduced_split_feats).intersection(set(reduced_gain_feats))))\n    union_set = sorted(list(set(reduced_split_feats).union(set(reduced_gain_feats))))\n    print(len(reduced_split_feats), len(reduced_gain_feats), len(inters), len(union_set))\n    print(f'Results for threshold {step}')\n    split_results = score_feature_selection(df=tourney_result, train_features=reduced_split_feats, cat_feats=None, target=tourney_result['result'], use_multiple_years=[2015, 2017, 2019])\n    print(f'split set: {split_results}')\n    gain_results = score_feature_selection(df=tourney_result, train_features=reduced_gain_feats, cat_feats=None, target=tourney_result['result'], use_multiple_years=[2015, 2017, 2019])\n    print(f'gain set: {gain_results}')\n    try:\n        intersection_results = score_feature_selection(df=tourney_result, train_features=inters, cat_feats=None, target=tourney_result['result'], use_multiple_years=[2015, 2017, 2019])\n        print(f'intersection set: {intersection_results}')\n    except ValueError:\n        pass\n    union_results = score_feature_selection(df=tourney_result, train_features=union_set, cat_feats=None, target=tourney_result['result'], use_multiple_years=[2015, 2017, 2019])\n    print(f'union set: {union_results}')\n\n    if split_results < base_score:\n        reduced_feats = reduced_split_feats\n        base_score = split_results\n    if gain_results < base_score:\n        reduced_feats = reduced_gain_feats\n        base_score = gain_results\n    if intersection_results < base_score:\n        reduced_feats = inters\n        base_score = intersection_results\n    if union_results < base_score:\n        reduced_feats = union_set\n        base_score = union_results\nprint(base_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_f_std = null_imp_df.groupby('feature')['importance_gain'].std().reset_index()\nnull_f_mean = null_imp_df.groupby('feature')['importance_gain'].mean().reset_index()\nact_f_mean = actual_imp_df.groupby('feature')['importance_gain'].mean().reset_index()\nact_f_mean['p values gain'] = 1 - norm.cdf(act_f_mean['importance_gain'], null_f_mean['importance_gain'], null_f_std['importance_gain'])\nnull_f_std = null_imp_df.groupby('feature')['importance_split'].std().reset_index()\nnull_f_mean = null_imp_df.groupby('feature')['importance_split'].mean().reset_index()\nact_f_mean_spl = actual_imp_df.groupby('feature')['importance_split'].mean().reset_index()\nact_f_mean['p values split'] = 1 - norm.cdf(act_f_mean_spl['importance_split'], null_f_mean['importance_split'], null_f_std['importance_split'])\n\nreduced_feats = act_f_mean.loc[act_f_mean['p values gain'] < 0.05, 'feature'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(reduced_feats), len(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another parameter search, with the optimal features.\n\ndef get_opt_params(init_points, n_iter, use_multiple_years=None):\n    \n    def run_parameter_opt(num_leaves, feature_fraction, bagging_fraction, min_data_in_leaf, max_depth, lambda_l1, lambda_l2,\n                          min_split_gain, min_child_weight, learning_rate):\n        params = {'num_leaves': int(num_leaves),\n              'min_child_weight': min_child_weight,\n              'min_split_gain' : min_split_gain,\n              'feature_fraction': feature_fraction,\n              'bagging_fraction': bagging_fraction,\n              'min_data_in_leaf': int(min_data_in_leaf),\n              'max_depth': int(max_depth),\n              'learning_rate': learning_rate,\n              'lambda_l1': lambda_l1,\n              'lambda_l2': lambda_l2,\n              'random_state': 7,\n              'boosting_type': 'gbdt',\n              'bagging_seed': 0,\n        }\n        if not use_multiple_years:\n            model = NCAA_model(params, tourney_result, test_df, use_holdback=[2019], regression=False, verbose=False)   \n            tr_score, oof_score, hb_score = model.train(reduced_feats, n_splits=10, n_boost_round=10000, stopping_rounds=100)\n            spline, spline_s = model.fit_spline_model(verbose=False)\n            return -spline_s\n        \n        spline_scores = []\n        hb_scores = []\n        for year in use_multiple_years:\n            model = NCAA_model(params, tourney_result, test_df, use_holdback=[year], regression=False, verbose=False)   \n            tr_score, oof_score, hb_score = model.train(reduced_feats, n_splits=10, n_boost_round=10000, stopping_rounds=100)\n            spline, spline_s = model.fit_spline_model(verbose=False)\n            spline_scores.append(spline_s)\n            hb_scores.append(hb_score)            \n        return -np.mean(hb_scores)\n\n    lgbBO = BayesianOptimization(run_parameter_opt, {'num_leaves': (15, 1200),\n                                        'feature_fraction': (0.5, 1),#.4515\n                                        'bagging_fraction': (0.4, 1),\n                                        'min_data_in_leaf': (20, 300),\n                                        'max_depth': (-1, 35),\n                                        'lambda_l1': (0, 3.5),\n                                        'lambda_l2': (0, 3.5),\n                                        'min_split_gain': (0, 0.3),\n                                        'min_child_weight': (0, 0.5),\n                                        'learning_rate': (0.001, 0.75)})\n    lgbBO.maximize(init_points=init_points, n_iter=n_iter)\n    \n    return lgbBO.max['params']\n\nparams = get_opt_params(init_points=12, n_iter=48, use_multiple_years=[2015, 2017, 2019])\nparams['random_state'] = 7\nparams['boosting_type'] = 'gbdt'\nparams['bagging_seed'] =  0\nparams['min_data_in_leaf'] =  int(params['min_data_in_leaf'])\nparams['max_depth'] =  int(params['max_depth'])\nparams['num_leaves'] =  int(params['num_leaves'])\n\n#{'bagging_fraction':0.6316, 'feature_fraction':0.2083, 'lambda_l1':0.4754, 'lambda_l2':0.5603, 'learning_rate':0.4492, 'max_depth':1,\n#'min_child_in_leaf':0.1958, 'min_data_in_leaf':33, 'min_split_gain':0.1215, 'num_leaves':310}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd2e4e23-00fc-46de-8391-8b797edd9d98","_cell_guid":"19b7dab9-5649-468f-9bf4-e6cc9faffe02","trusted":true},"cell_type":"code","source":"# Fit the final model.  Instead of early stopping using only RMSE, I ran the model and checked the validation score after using a smoothing spline.  \n\nstep_size = 5\nsteps = 50\nboosting_rounds = [step_size*(x+1) for x in range(steps)]\ndef run_boost_round_test(boosting_rounds, step_size):\n    training_scores, oof_scores, holdback_scores = [], [], []\n    model = NCAA_model(params, tourney_result, test_df, use_holdback=[2017], regression=False, verbose=False)   \n    print(f'Training for {step_size*steps} rounds.')\n    for rounds in range(step_size,boosting_rounds[-1]+1,step_size):\n        print(f'{\"*\"*50}')\n        print(f'Rounds: {rounds}')\n        if model.use_holdback:\n            tr_score, oof_score, hb_score = model.train(reduced_feats, n_splits=10, n_boost_round=step_size, stopping_rounds=None)\n        else:\n            tr_score, oof_score = model.train(reduced_feats, n_splits=10, n_boost_round=step_size, stopping_rounds=None)\n        clips, clip_s = model.fit_clipper(verbose=True)\n        spline, spline_s = model.fit_spline_model(verbose=True)\n        \n        training_scores.append([tr_score, model.postprocess_preds(clips, use_data = 'train'), \n                               model.postprocess_preds(spline, use_data = 'train', method='spline')])\n        oof_scores.append([oof_score, model.postprocess_preds(clips, use_data = 'oof'),\n                          model.postprocess_preds(spline, use_data = 'oof', method='spline')])\n        holdback_scores.append([hb_score, model.postprocess_preds(clips, use_data = 'hb'),\n                               model.postprocess_preds(spline, use_data = 'hb', method='spline')])\n\n    training_scores, oof_scores, holdback_scores\n    fig,ax = plt.subplots(nrows=1,ncols=3, figsize=(20,5), sharey=True, sharex=True)\n    plot_df = pd.DataFrame(data=training_scores, columns=['Classifier', 'Clipped', 'Spline'], index=boosting_rounds)\n    plot_df.plot(ax=ax[0], title='Training')\n    plot_df = pd.DataFrame(data=oof_scores, columns=['Classifier', 'Clipped', 'Spline'], index=boosting_rounds)\n    plot_df.plot(ax=ax[1], title='Out of Fold')\n    plot_df = pd.DataFrame(data=holdback_scores, columns=['Classifier', 'Clipped', 'Spline'], index=boosting_rounds)\n    plot_df.plot(ax=ax[2], title='Holdback')\n        \nrun_boost_round_test(boosting_rounds, step_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions","execution_count":null},{"metadata":{"_uuid":"3cb2dc39-9194-4f4e-86bb-7f93cec589a1","_cell_guid":"757180b2-fda6-40e3-8a7e-2aadfdc9435c","trusted":true},"cell_type":"code","source":"# Generate predictions on final model.  First fits a smoothing spline on a model train on a reduced training set.\n\nmodel = NCAA_model(params, tourney_result, test_df, use_holdback=[2015, 2017, 2019], regression=False, verbose=False)   \ntr_score, oof_score, _ = model.train(reduced_feats, n_splits=10, n_boost_round=1500, stopping_rounds=100)\nspline, spline_s = model.fit_spline_model(verbose=True)\n\nmodel = NCAA_model(params, tourney_result, test_df, use_holdback=False, regression=False, verbose=False)   \ntr_score, oof_score = model.train(reduced_feats, n_splits=10, n_boost_round=1500, stopping_rounds=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission","execution_count":null},{"metadata":{"_uuid":"32fb1893-73a0-4938-8639-e2751ac09462","_cell_guid":"6a8523aa-14b6-4be3-bfb6-d154dba69972","trusted":true},"cell_type":"code","source":"# Submit predictions\n\ny_preds = model.postprocess_preds(spline, method='spline')\nsubmission_df = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MSampleSubmissionStage1_2020.csv')\nsubmission_df['Pred'] = np.clip(y_preds, 0.0001, 0.9999)\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df.describe()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}